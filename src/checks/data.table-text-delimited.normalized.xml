<?xml version="1.0" encoding="UTF-8"?>
<mdq:check xmlns:mdq="https://nceas.ucsb.edu/mdqe/v1"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="https://nceas.ucsb.edu/mdqe/v1 ../schemas/schema1.xsd">
  <id>data.table-text-delimited.normalized</id>
  <name>Check whether the data is normalized.</name>
  <description>Data is normalized when there are no duplicate columns or rows and does not contain duplicate column or row names </description>
  <type>identification</type>
  <level>INFO</level>
  <environment>python</environment>
  <code><![CDATA[

def call():
    global output
    global status
    global output_identifiers
    global output_type
    global metadigpy_result

    from metadig import StoreManager
    from metadig import metadata as md
    import pandas as pd
    import io

    manager = StoreManager(storeConfiguration)  

    output_data = []
    status_data = []
    output_identifiers = []
    output_type = []
    metadigpy_result = {}

    if len(dataPids) == 0:
      output_data = "No data objects found."

    for pid in dataPids:
        output_identifiers.append(pid)

        # If data object is not available, skip the pid.
        try:
          # if file is not text/csv, skip it
          # otherwise get the object and filename
          obj, fname, csv_status = md.get_valid_csv(manager, pid)
          if csv_status == "SKIP":
              output_data.append(f"{fname} is not a text-delimited table, skipping.")
              output_type.append("text")
              status_data.append(csv_status)
              continue
        except Exception as e:
            output_data.append(f"Unexpected Exception: {e}")
            output_type.append("text")
            status_data.append("FAILURE")
            continue

        # Ensure that the data object is documented in the list of entity names
        entity_index = md.find_entity_index(fname, pid, entityNames, ids)[0]
        if entity_index is None:
            output_data.append(f"{fname} does not appear to be documented in the metadata.")
            output_type.append("text")
            status_data.append("FAILURE")
            continue

        # Set the default expecated header line value (the first row is headers)
        num_header_lines = 0
        # headerLines represents 'numHeaderLines' which is retrieved from the EML document
        if isinstance(headerLines, list):
            # If we successfully retrieve a list, we will check what the value is
            header_line_value = headerLines[entity_index]
            if header_line_value == 'null':
                # There are no header lines
                pass
            if isinstance(header_line_value, int):
                # We'll use the value from the list of it is an integer
                num_header_lines = header_line_value
        
        # Try to read the data object
        obj_bytes_read = obj.read()
        encoding_type, enc_msg = md.detect_text_encoding(obj_bytes_read)
        d_read_decoded = obj_bytes_read.decode(encoding_type)
        df, error = md.read_csv_with_metadata(d_read_decoded, fieldDelimiter[entity_index], num_header_lines, encoding_type)
        if error:
            output_data.append(f"{fname} is unable to be read as a table. {error}")
            output_type.append("text")
            status_data.append("FAILURE")
            continue

        # If any errors are found, the data will not pass the normalized check
        normalization_errors_count = 0
        normalization_errors_data = []

        # Check for duplicate field names
        duplicate_cols, contains_period = md.find_duplicate_column_names(df)
        if duplicate_cols:
            normalization_errors_count += 1
            normalization_errors_data.append(f"{fname} contains duplicate columns: {', '.join(duplicate_cols)}")
        elif contains_period:
            # Add a warning if there's no duplicates and a period is found in the field
            warn_msg = f"{fname} has periods '.' in the field names. We recommend replacing with underscores '_'"
            normalization_errors_data.append(warn_msg)

        # Check for duplicate column content
        duplicate_col_content = md.find_duplicate_column_content(df)
        if len(duplicate_col_content) != 0:
            normalization_errors_count += 1
            error_msg = "; ".join(
                f"'{dup}' duplicates '{orig}' (hash: {h})"
                for dup, orig, h in duplicate_col_content
            )
            normalization_errors_data.append(
                f"{fname} contains duplicate column content: {error_msg}"
            )

        # Check for duplicate rows
        duplicate_rows = md.find_duplicate_rows(df)
        if duplicate_rows is not None:
            dr_summary = duplicate_rows.to_markdown()
            normalization_errors_count += 1
            normalization_errors_data.append(
                f"{fname} contains duplicate rows: \n{dr_summary} \n"
            )

        # Check for obscene amount of columns
        num_of_cols = md.find_number_of_columns(df)
        if num_of_cols >= 1000:
            normalization_errors_count += 1
            normalization_errors_data.append(
                f"{fname} contains an excessive number of columns: {num_of_cols}"
            )

        # TODO: DISCUSS - Adding check for reasonable amount of non-repeating values

        norm_errors_md_output = '\n'.join(f'| {line}' for line in normalization_errors_data)
        if normalization_errors_count == 0:
            if normalization_errors_data:
                output_data.append(f"{fname} is normalized. Additional Details: {norm_errors_md_output}")
            else:
                output_data.append(f"{fname} is normalized.")
            output_type.append("markdown")
            status_data.append("SUCCESS")
        else:
            output_data.append(f"{fname} contains normalization issues: {norm_errors_md_output}")
            output_type.append("markdown")
            status_data.append("FAILURE")

    successes = sum(x == "SUCCESS" for x in status_data)
    failures = sum(x == "FAILURE" for x in status_data)
    skips = sum(x == "SKIP" for x in status_data)
    output = output_data
    if successes > 0 and failures == 0:
        status = "SUCCESS"
    elif successes == 0 and failures > 0:
        status = "FAILURE"
    else:
        status = "FAILURE" 

    metadigpy_result["identifiers"] = output_identifiers
    metadigpy_result["output"] = output_data
    metadigpy_result["status"] = status
    return True

  ]]></code>
  <selector>
    <name>entityNames</name>
    <xpath>/eml/dataset/*[self::dataTable|self::otherEntity]</xpath>
    <subSelector>
      <name>...</name>
      <xpath>./entityName</xpath>
    </subSelector>
  </selector>
  <selector>
    <name>objectNames</name>
    <xpath>/eml/dataset/*[self::dataTable|self::otherEntity]</xpath>
    <subSelector>
      <name>...</name>
      <xpath>./physical/objectName</xpath>
    </subSelector>
  </selector>
  <selector>
    <name>ids</name>
    <xpath>/eml/dataset/*[self::dataTable|self::otherEntity]/@id</xpath>
  </selector>
  <selector>
     <name>fieldDelimiter</name>
    <xpath>/eml/dataset/*[self::dataTable|self::otherEntity]</xpath>
     <subSelector>
        <name>...</name>
        <xpath>./physical/dataFormat/textFormat/simpleDelimited/fieldDelimiter</xpath>
    </subSelector>
  </selector>
  <selector>
     <name>headerLines</name>
    <xpath>/eml/dataset/*[self::dataTable|self::otherEntity]</xpath>
     <subSelector>
        <name>...</name>
        <xpath>./physical/dataFormat/textFormat/numHeaderLines</xpath>
    </subSelector>
  </selector>
  <dialect>
    <name>Ecological Metadata Language</name>
    <xpath>boolean(/*[local-name() = 'eml'])</xpath>
  </dialect>
</mdq:check>
